import streamlit as st
import requests
import json


st.set_page_config(page_title="Gemini Grounding Demo", page_icon="ğŸ”")
st.title("ğŸ” Grounding powered by Gemini")


# --- APIã‚­ãƒ¼ã®è¨­å®š ---
try:
    api_key = st.secrets["GEMINI_API_KEY"]

except FileNotFoundError:
    # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒãªã©ã§ .streamlit/secrets.toml ãŒãªã„å ´åˆã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
    st.error("APIã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚.streamlit/secrets.toml ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    st.stop()


# --- Streamlit UI ---
prompt = st.text_input(
    "è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:", "2025å¹´ã®è‡ªæ°‘å…šç·è£é¸æŒ™ã§å‹åˆ©ã—ãŸã®ã¯èª°ã§ã™ã‹ï¼Ÿ"
)


if st.button("ç”Ÿæˆã™ã‚‹"):
    if not prompt:
        st.warning("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    else:
        with st.spinner("Googleæ¤œç´¢ã‚’å‚ç…§ã—ã¦å›ç­”ã‚’ç”Ÿæˆä¸­..."):
            # ãƒ¢ãƒ‡ãƒ«ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆURL

            url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?key={api_key}"

            # APIã«é€ä¿¡ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ï¼‰

            payload = {
                "contents": [{"parts": [{"text": prompt}]}],
                "tools": [{"googleSearch": {}}],
            }

            headers = {"Content-Type": "application/json"}

            try:
                response = requests.post(url, headers=headers, data=json.dumps(payload))

                response.raise_for_status()  # HTTPã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Œã°ä¾‹å¤–ã‚’ç™ºç”Ÿã•ã›ã‚‹

                response_json = response.json()

                generated_text = response_json["candidates"][0]["content"]["parts"][0][
                    "text"
                ]

                st.markdown(generated_text)

            except requests.exceptions.HTTPError as http_err:
                st.error(f"HTTPã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {http_err}")

                st.error(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹å†…å®¹: {response.text}")

            except Exception as e:
                st.error(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    # --- ãƒ‡ãƒãƒƒã‚°æƒ…å ± ---
    st.header("ãƒ‡ãƒãƒƒã‚°æƒ…å ±")
    with st.expander("APIãƒ¬ã‚¹ãƒ³ã‚¹ã®JSONå…¨ä½“ã‚’è¡¨ç¤º"):
        st.json(response_json)

    # --- ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°è©³ç´° ---
    grounding_meta = response_json.get("candidates", [{}])[0].get(
        "groundingMetadata", {}
    )

    if grounding_meta:  # groundingMetadataãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿è¡¨ç¤º
        with st.expander("ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°è©³ç´°ï¼ˆæ¤œç´¢ã‚¯ã‚¨ãƒªã¨å‚ç…§ã‚½ãƒ¼ã‚¹ï¼‰"):
            # æ¤œç´¢ã‚¯ã‚¨ãƒªã®è¡¨ç¤º
            st.subheader("ğŸ¤– AIãŒå®Ÿè¡Œã—ãŸæ¤œç´¢ã‚¯ã‚¨ãƒª")
            search_queries = grounding_meta.get("webSearchQueries", [])
            if search_queries:
                for query in search_queries:
                    st.info(query)
            else:
                st.write("æ¤œç´¢ã‚¯ã‚¨ãƒªã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

            st.markdown("---")

            # ä½¿ç”¨ã—ãŸã‚½ãƒ¼ã‚¹ã®ä¸€è¦§è¡¨ç¤º
            st.subheader("ğŸ“š å‚ç…§ã—ãŸWebã‚½ãƒ¼ã‚¹ä¸€è¦§")
            grounding_chunks = grounding_meta.get("groundingChunks", [])
            if grounding_chunks:
                for i, chunk in enumerate(grounding_chunks):
                    title = chunk.get("web", {}).get("title", "ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜")
                    uri = chunk.get("web", {}).get("uri", "#")
                    # Markdownã‚’ä½¿ã„ã€ã‚¿ã‚¤ãƒˆãƒ«ã‚’ã‚¯ãƒªãƒƒã‚¯å¯èƒ½ãªãƒªãƒ³ã‚¯ã«ã™ã‚‹
                    st.markdown(f"{i + 1}. **{title}** - [ãƒªãƒ³ã‚¯]({uri})")
            else:
                st.write("å‚ç…§ã‚½ãƒ¼ã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    # --- ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯è¡¨ç¤º ---
    st.header("ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯è¡¨ç¤º")
    st.caption("AIãŒç”Ÿæˆã—ãŸå„æ–‡ç« ã¨ãã®æ ¹æ‹ ã¨ãªã£ãŸã‚½ãƒ¼ã‚¹")

    grounding_supports = grounding_meta.get("groundingSupports", [])

    if not grounding_supports:
        st.warning("æ ¹æ‹ æƒ…å ±(groundingSupports)ãŒãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã§ã—ãŸã€‚")
    else:
        # å–å¾—ã—ãŸæƒ…å ±ã‚’ãƒ«ãƒ¼ãƒ—ã§è¡¨ç¤º
        for gs in grounding_supports:
            # st.container(border=True) ã§å„æƒ…å ±ã‚’ã‚«ãƒ¼ãƒ‰åŒ–ã™ã‚‹
            with st.container(border=True):
                # 1. AIãŒç”Ÿæˆã—ãŸæ–‡ç« éƒ¨åˆ†
                text_segment = gs.get("segment", {}).get(
                    "text", "ãƒ†ã‚­ã‚¹ãƒˆãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
                )
                st.markdown(text_segment.strip())

                st.markdown("---")  # æ°´å¹³ç·šã§åŒºåˆ‡ã‚‹

                # 2. ãã®æ–‡ç« ã®è£ä»˜ã‘ã¨ãªã£ãŸã‚½ãƒ¼ã‚¹
                st.caption("ã‚½ãƒ¼ã‚¹")

                indices = gs.get("groundingChunkIndices", [])
                if not indices:
                    st.write("ã“ã®æ–‡ç« ã«å¯¾å¿œã™ã‚‹ã‚½ãƒ¼ã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                else:
                    for gci in indices:
                        # grounding_chunksãƒªã‚¹ãƒˆã‹ã‚‰è©²å½“ã™ã‚‹ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’å–å¾—
                        source_chunk = grounding_chunks[gci]
                        title = source_chunk.get("web", {}).get("title", "ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜")
                        uri = source_chunk.get("web", {}).get("uri", "#")
                        # ã‚¢ã‚¤ã‚³ãƒ³ã‚’ä»˜ã‘ã¦åˆ†ã‹ã‚Šã‚„ã™ãè¡¨ç¤º
                        st.markdown(f"ğŸ”— **{title}** - [ã‚½ãƒ¼ã‚¹ã¸]({uri})")
